\input{preamble}

\title{}
\author{Козырнов Александр Дмитриевич, ИУ7-32Б}
\date{\today}

\begin{document}

\chapter{Del}

\section{Многомерная интерполяция}

Пусть $z = f(x, y)$. Работаем в трехмерном пространстве.


Обычно представляют это в виде таблицы x,y. На регулярной сетке многомерная интерполяция выполняется
последовательно (по строкам и по столбцам). Задаются отдельно степени полиномов $n_x$ и  $n_y$.
Относительно  $x$ берем  $n_x +1$ строк, а относительно  $y$ берем  $n_y + 1$ столбцов.

$$
\left
\begin{array}
    zz = f(x,y)\\
    z_0 = f(x_1,y)\\
    z_1 = f(x_1, y)\\
    z_2 = f(x_2,y)
\end{array}
\right\} \to z = f(x,y)
$$

\medskip

Если $u = f(x,y,z)$, то при каждой новой переменной алгоритм сводится к интерполяции функции на одну
переменную меньше. То есть
 \[
u = f(x, y, z_i)
\] 
$$
\begin{matrix}
    z & u\\
    z_0 & u_0\\
    \vdots & \vdots\\
    z_n & u_n
\end{matrix}
$$


\subsection{Двумерный полином Ньютона}
Это когда матрица (таблица) треугольного вида.

\[
\scr{P}_n(x,y) = \sum\limits_{i=0}^{n}\sum\limits_{j=0}^{n-i} z(x_0,x_1,\ldots,x_{i}; y_0,y_1,
\ldots,y_{j})\prod\limits_{p=0}^{i-1}(x-x_p)\prod\limits_{q=0}^{j-1}(y-y_q)   

\medskip

z - разделенная разность.

\begin{gathered}
    z(x_0,x_1,y_0) = \frac{z(x_0,y_0) - z(x_1,y_0)}{x_0-x_1}\\
    z(x_0,x_1;y_0,y_1) = \frac{z(x_0,x_1,y_0) - z(x_0,x_1,y_1)}{y_0-y_1}
\end{gathered}
\] 

\paragraph*{Замечание.} ${}$ \newline

При вычислении полинома Ньютона погрешность вычисляется такой формулой:
\[
\left| y - \scr{P}_n(x) \right| \le \frac{M_n + 1}{(n+1)!}\left| \omega_{n+1}(x) \right| 
\] 
где $M_{n+1} = max(y^{(n+1)})$, $\omega_{n+1}(x) = \prod\limits_{i=0}^{x_0\le x\le xn} (x- x_{i})$

\chapter{Наилучшее среднее квадратичное приближение}

Чем меньше ошибка, тем важнее точка. Отчего мы присваиваем вес каждой точке. Вес - $\rho$

 \[
\begin{matrix}
    x_{i} & y_{i} & \rho_i\\
    \vdots & \vdots & \vdots\\
    x_{n} & y_{n} & \rho_n\\
\end{matrix}
\] 

$\rho$ может быть задан пользователем. Иначе считаем сами как обратная от ошибки:
 \[
\rho_i \sim \frac{1}{\epsilon_i^2}
\] 

Задача искания аппроксимирующей функции способом усреднения набором данных формулируется
следующим образом. Вводится такая величина $I = \sum\limits \rho_i[y_{i} - \phi(x_{i})]^2$ - 
сумма квадратов отклонения.

Тогда задачи. Подобрать $\phi(x)$ так, чтобы
\begin{itemize}
    \item[1)] $I < \epsilon$, то есть сумма меньше погрешности
    \item[2)] $I \to I_{min}$
\end{itemize}

Можно записать так (общий вариант, но пользоваться не будем, тк у нас дискретный вариант):
\[
I = \int\limits_{a}^{b}\rho(x)(y(x) - \phi(x))^2dx
\] 

Пользоваться мы будем методом наименьших квадратов.

\medskip

Рассмотрим задачу построение наилучшего квадратичного приближения (метод наименьших квадратов).
Выберем $\phi(x)$ так, чтобы она была линейной.
 \[
     \phi(x) = \sum\limits_{k=0}^{n} a_k\phi_k(x) \quad\quad \phi_k(x)\text{ - известные линейно
     независимые функции}
\]
Чаще всего $\phi_k(x) = x^{k}$. Они называются базисными функциями. Отсюда
\[
    I = \sum\limits_{i}\rho_i\left[y_{i} - \sum\limits_{k=0}^{n}a_k\phi_k(x)\right]^2
\]

\medskip

Предварительно рассмотрим скалярное произведение двух функций:
\[
    (f, \phi) = \sum\limits_{i=1}^{N}\rho_i f(x_{i}) \cdot \phi(x_{i}) 
\] 

Свойства скалярного произведения:
\begin{itemize}
    \item[1)] $(f,\phi) = (\phi, f)$
    \item[2)] $(\alpha f, \phi) = \alpha(f,\phi)$ (следствие $(f,\alpha\phi) = \alpha(f,\phi)$)
    \item[3)] $(f + \phi, \psi) = (f, \psi) + (\phi, \psi)$
\end{itemize}
Тогда
\[
I = (y - \phi, y - \phi) = (y,y) - 2(y, \phi) + (\phi, \phi) = (y, y) - 2\left(y, \sum\limits_{k=0}^{n}
                        a_k\phi_k(x)\right) + \left(\sum\limits_{k=0}^{n}a_k\phi_k(x),
                        \sum\limits_{m=0}^{n}a_m\phi_m(x)\right)
\]

В итоге получим
\[
I = (y, y) - 2 \sum\limits_{k=0}^{n}a_k\left(y, \phi_k(x)\right) +
\sum\limits_{k=0}^{n}\sum\limits_{m=0}^{n}a_k a_m(\phi_k,\phi_m)  
\]

Осталось минимизировать $I$. Значит, ищем неизвестные параметры.
 \[
     \pdx{I}{a_k} = 0,\quad k = \overline{0,n}
\]

\medskip

Получим
\[
    \pdx{I}{a_k} = -2(y,\phi_k) + 2 \sum\limits_{m=0}^{n}a_m(\phi_k,\phi_m) = 0 
\] 

Итого система состоит из
\[
\sum\limits_{m=0}^{n}a_m(\phi_k,\phi_m) = (y,\phi_k), \quad k = \overline{0,n} 
\]
что мы и искали. Определитель тут Грамма, и он не равен нулю. Наилучшее среднее приближение
существует и оно единственно по этой причине.

Если $\phi_k$ и  $\phi_m$ ортогональны, то получаем
\begin{equation}
a_k = (y, \phi_k) = \sum\limits_{i=1}^{N}\rho y_i \phi_k(x_{i}), \quad k = \overline{0,n} 
\end{equation}
В таком случае (если базис ортогонален), то это ряд Фурье.

\paragraph*{Заметка.}
Нам всегда известны $y_i, \phi_k, \rho_i$.


\medskip

Часто во многих задачах используется $\phi_k(x) = x^{k}$. Тогда
\[
    (\phi_k, \phi_m) = \sum\limits_{i=0}^{N}\rho_i x_{i}^{k} x_{i}^{m} = \sum\limits_{i=1}^{N}\rho_i
    x_{i}^{k+m}
\]

\[
    (y, \phi_k) = \sum\limits_{i=0}^{N}\rho_i y_{i} x_{i}^{k} 
\] 

\paragraph*{Пример.} ${}$ \newline

 $\phi_k(x) = x^{k}$ 

 $n = 1$

 Аппроксимирующая функция:  $\phi(x) = a_0 + a_1x$

Тогда для формулы (1):
\[
\begin{cases}
    a_0 (\phi_0, \phi_0) + a_1(\phi_0,\phi_1) = (y,\phi_0) - k = 0\\
    a_0(\phi_1, \phi_0) + a_1(\phi_1,\phi_1) = (y, \phi_1) - k = 1
\end{cases}
\]

\[
\begin{cases}
    a_0 \sum\limits_{i=0}^{N}\rho_i + a_1 \sum\limits_{i=1}^{N}\rho_i x_{i} = \sum\limits_{i=0}^{N}
    \rho_i y_{i}\\
    a_0 \sum\limits_{i=0}^{N}\rho_i x_{i} + a_1 \sum\limits_{i=1}^{N}\rho_i x_{i}^2 =
    \sum\limits_{i=0}^{N} \rho_i y_{i} x_{i}\\
\end{cases}
\]
Отсюда найдем $a_0, a_1$.

\end{document}
